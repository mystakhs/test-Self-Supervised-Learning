{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mystakhs/test-Self-Supervised-Learning/blob/main/VideoMAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VideoMAEの動作確認\n",
        "目的：固定カメラ動画から 組み立て作業クラス を自動取得できるか 短時間 GPU で検証\n",
        "戦略：\n",
        "\n",
        "事前学習済み VideoMAE-Base（または公開が出次第 V-JEPA Tiny）を 特徴抽出器として凍結\n",
        "\n",
        "数十本だけ手ラベルしたクリップで 線形ヘッド を微調整 (≒ 30 min on T4)\n",
        "## 2-1. Google Colab 環境セットアップ"
      ],
      "metadata": {
        "id": "F-Cyor9vRLcu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw07D_cmPqGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31f40712-e437-49c0-a2dd-a601532b76ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Colab 起動セル\n",
        "!pip install -q transformers==4.40 timm decord einops accelerate datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-2. 最小サンプルデータ"
      ],
      "metadata": {
        "id": "s4J2Z3okRtWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Tiny-Kinetics-400から5クラスだけ取得\n",
        "ds = load_dataset(\"datalab/Tiny-Kinetics-400\", split=\"train[:50]\")\n",
        "\n",
        "# クラス名を確認して、適当な2クラスだけ使ってもよい\n",
        "ds = ds.filter(lambda x: x[\"label\"] in [10, 20])  # 適宜クラスID調整\n"
      ],
      "metadata": {
        "id": "Cwe5aGc9ScRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "# UCF101 から 2 クラス (e.g., 'PushUps', 'JumpingJack') だけ取得し 10 本ずつ\n",
        "# ds = load_dataset(\"ucf101\", \"bringToOther\", split=\"train[:20]\")\n",
        "ds = load_dataset(\"flwrlabs/ucf1011\", split=\"train[:20]\")\n",
        "\n",
        "# 独自工場動画は drive に mp4 を置き、load_dataset で VideoFolder 形式でも OK\n"
      ],
      "metadata": {
        "id": "Rf4Wil15QxUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-3. モデル & 前処理"
      ],
      "metadata": {
        "id": "mKKtohSFRKOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
        "import torch, torch.nn as nn\n",
        "\n",
        "id_pretrain = \"MCG-NJU/videomae-base-finetuned-kinetics\"  # HF ckpt :contentReference[oaicite:6]{index=6}\n",
        "processor = VideoMAEImageProcessor.from_pretrained(id_pretrain)\n",
        "model = VideoMAEForVideoClassification.from_pretrained(id_pretrain)\n",
        "\n",
        "# ↓ 軽量 PoC: エンコーダ凍結し線形層だけ再定義\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "num_cls = 2                      # ← 作業ステップ数に合わせる\n",
        "model.classifier = nn.Linear(model.config.hidden_size, num_cls)\n"
      ],
      "metadata": {
        "id": "Gr4zRviqQ01s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-4. データローダ（16 frames, 112² px で省メモリ）"
      ],
      "metadata": {
        "id": "xFu9GY5oRyE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import decord, torchvision.transforms as T\n",
        "decord.bridge.set_bridge('torch')\n",
        "\n",
        "tube_size = 16\n",
        "img_tf = T.Compose([\n",
        "    T.Resize(128),\n",
        "    T.CenterCrop(112),\n",
        "    T.Normalize([0.45], [0.225])\n",
        "])\n",
        "\n",
        "def collate(batch):\n",
        "    videos = [b[\"video\"][:tube_size].permute(0,3,1,2) / 255. for b in batch]\n",
        "    videos = torch.stack([img_tf(v) for v in videos])            # [B, T, C, H, W]\n",
        "    labels = torch.tensor([b[\"label\"] for b in batch])\n",
        "    return {\"pixel_values\": videos, \"labels\": labels}"
      ],
      "metadata": {
        "id": "-6QNxwZcQ3_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-5. 15 分で線形プローブ"
      ],
      "metadata": {
        "id": "hL_6jo1LR1-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "args = TrainingArguments(\n",
        "    \"videomae_poc\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    fp16=True,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\"\n",
        ")\n",
        "trainer = Trainer(model, args, train_dataset=ds, eval_dataset=ds, data_collator=collate)\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "chGGz2iKQ6sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "メモリ目安: T4 GPU (15 GB) で < 5 GB 使用。学習は ≈ 2 it/s → 3 epoch で 10 min 程度。"
      ],
      "metadata": {
        "id": "QFWCoVD3R4tG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-6. 予測 & 作業時間計測"
      ],
      "metadata": {
        "id": "FsJ-QUW2R5k4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, cv2\n",
        "def predict_clip(path):\n",
        "    vr = decord.VideoReader(path, num_threads=1)\n",
        "    frames = torch.tensor(vr.get_batch(range(0,len(vr), max(1, len(vr)//16))))\n",
        "    inputs = processor(frames.permute(0,3,1,2), return_tensors=\"pt\")\n",
        "    logits = model(**inputs).logits\n",
        "    return logits.softmax(-1).argmax(-1).item()\n",
        "\n",
        "# ストリーム全体を 2 秒ごとに切り出し → 連続同ラベル区間を統計して作業時間に変換\n"
      ],
      "metadata": {
        "id": "cBFGZWyoQ6pS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-7. V-JEPA を試す場合"
      ],
      "metadata": {
        "id": "QjS-ye5pSAjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q jinaai-jeparch  # *仮*: 近い公開実装\n",
        "from jepa import VJEPAEncoder        # Tiny weight (128 × 16) を使用\n",
        "# エンコーダ出力を上記線形ヘッドに差し替えるだけ\n"
      ],
      "metadata": {
        "id": "dkzTQQHNQ6l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "V-JEPA は マスク不要×高速 なので凍結特徴抽出なら VideoMAE と同コスト。公開 weight が少ないため、まず VideoMAE Tiny で PoC→将来置換が現実的。"
      ],
      "metadata": {
        "id": "SBFyh90WSHd3"
      }
    }
  ]
}