
# 🧠 Image Matchingに必要な理論

## 📂 データセット構造
- 各データセットは複数のシーンで構成されています。
- シーンとは、同じオブジェクトの異なる視点など、関連する画像のグループです。
- 一部の画像は外れ値（アウトライア）であり、どのシーンにも属しません。

**テストデータでのタスク:**

1. 画像をシーンを表すグループにクラスタリングする。
2. 各画像をクラスタに割り当てるか、外れ値としてラベル付けする。
3. 各画像（外れ値を除く）のカメラポーズ（R, T）を再構築する。

---

## 🧠 カメラポーズ推定とmAA

- カメラポーズは、回転行列（3×3）と並進ベクトル（3D）で表されます。
- トレーニングデータでは、真のカメラポーズ（グラウンドトゥルース）が提供されます。
- テストデータでは、これを予測する必要があります。

**カメラ中心Cの計算:**

$$
C = -R^T \cdot T
$$

これは、カメラポーズを3D空間の点に変換します。

**Structure from Motion (SfM):**

- 各画像のカメラポーズを再構築します。
- シーンのスパースな3D点群を生成します。

**予測されたカメラ中心の比較:**

- 予測されたカメラ中心Cとグラウンドトゥルースの中心Cgを比較するために、類似変換T（スケール、回転、並進）を適用します。
- これは、予測された3Dカメラ位置をグラウンドトゥルースに整列させます。

**類似変換の計算:**

- 3つの非共線な点の対応（トリプレット）を使用します。
- 各トリプレットには、予測された3D位置Ciと既知のグラウンドトゥルース位置Cgiが含まれます。

**RANSACアルゴリズム:**

- 多数のトリプレットをランダムにテストし、外れ値を無視して最適な変換を見つけます。

**Hornの方法:**

- 2つの3D点群間の最適な類似変換（スケールs、回転R、並進t）を計算します。

$$
T(C_i) = s \cdot R \cdot C_i + t
$$

**登録カメラの判定:**

$$
	ext{is\_registered} = (\| T(C_i) - Cg_i \| < 	ext{threshold})
$$

ここで、 $\| \cdot \|$ は2つの3D点間のユークリッド距離を表します。

**mAA（mean Average Accuracy）の計算:**

1. 各シーンに対して、複数の閾値 $t_j$ で登録カメラの割合 $r_j$ を計算します。
2. 各シーンの平均精度AAを求めます。
3. すべてのシーンのAAを平均して、データセット全体のmAAを計算します。

---

## 🧠 クラスタリングと最終評価スコア

**評価プロセス:**

- 予測されたクラスタ $C_{kj}$ とグラウンドトゥルースのシーン $S_{ki}$ を比較します。
- 各シーン $S_{ki}$ に対して、最も高いmAAを与えるクラスタ $C_{kji}$ を選択します。
- 外れ値としてラベル付けされた画像は、このマッチングステップから除外されます。

**クラスタリングスコア（精度）の計算:**

$$
	ext{clustering\_score} = rac{|S \cap C|}{|C|}
$$

ここで、 $|S \cap C|$ は予測されたクラスタと実際のシーンの重複する画像の数、 $|C|$ は予測されたクラスタ内の画像の総数です。

**ポーズスコア（mAA）の計算:**

- 各画像の予測されたカメラ中心をグラウンドトゥルースに整列させます。
- 各閾値で登録カメラの数をカウントし、登録率を計算します。
- 各シーンの登録率を平均してmAAを求めます。
- すべてのシーンのmAAを平均して、データセット全体のmAAを計算します。

**データセットごとの最終スコア:**

$$
	ext{final\_score\_per\_dataset} = rac{2 \cdot (	ext{mAA} \cdot 	ext{clustering\_score})}{	ext{mAA} + 	ext{clustering\_score}}
$$
これは、分類におけるF1スコアに類似しており、クラスタリングの精度とポーズの再現率の両方を考慮します。


**最終チャレンジスコア:**
すべてのデータセット$D_k$に対して評価を繰り返し、最終スコアは以下のように計算されます。
$$
	ext{mean}(	ext{final\_score\_per\_dataset})
$$

---

## 🧩 シーンクラスタリングとポーズ推定（SfM）

### 📌 1) シーンクラスタリング

#### 📘 1.1) グローバルディスクリプタによる教師なしクラスタリング
- CLIPやDINOv2などのモデルを使用して、画像のグローバル埋め込みを抽出します。
- DBSCAN、KMeans、階層クラスタリングなどのアルゴリズムでクラスタリングを行います。
##### グローバルディスクリプタ:
- 画像全体を表す単一のベクトルで、オブジェクト、テクスチャ、レイアウトなどを含みます。
##### 使用例:
- 画像クラスタリング
- 画像検索
- シーンクラス分類​
##### DINOv2:
- 自己教師ありのビジョントランスフォーマーで、異なる視点の画像の特徴ベクトルを一致させるように学習します。
- 手動のラベルを必要とせず、コントラスト学習を使用します。​

#### 🔧 グローバルディスクリプタクラスタリングの方法
1. 各画像からDINOv2などを使用して単一の埋め込みを抽出します。
2. すべての埋め込み間のペアワイズ距離（通常はコサイン距離）を計算します。
3. 以下のアルゴリズムでクラスタリングを行います:
 - DBSCAN: クラスタ数を指定せずに密度ベースでクラスタリング。
 - 階層クラスタリング: 階層的にグループ化。
 - KMeans: クラスタ数が既知の場合に使用。
 - グラフベースのクラスタリング（後述）。​

##### 結果:
- 各クラスタが予測されたシーンを表します。

#### 📘 グラフベースのクラスタリング

**パイプラインでの使用:**

1. DINOv2で抽出した画像埋め込み間のコサイン距離を計算します。
2. 以下の条件でグラフを構築します:
   - 各ノードは画像を表します。
   - 距離が閾値（例: 0.3）未満ならエッジを作成します。
3. このグラフの連結成分をそれぞれ予測されたシーンとみなします。
4. エッジを持たない孤立ノードは外れ値として扱います。

**注意:**  
この段階ではALIKEDやLightGlueは使わず、これらは後段の局所特徴抽出・マッチング、つまり3Dカメラポーズ再構築（Structure-from-Motion, SfM）の際に使用します。

---

## 🔍 ローカルキーポイントとディスクリプタ（一般理論）

- キーポイントとは視点や照明の変化に安定な、特徴的な画像領域（コーナー、エッジ、ブロブなど）です。
- ローカルディスクリプタはキーポイント周辺を記述する特徴ベクトルであり、異なる画像間で対応付けを行います。

**代表的な手法:**

- 手工設計: SIFT, ORB, AKAZE
- 学習ベース: SuperPoint, D2Net, R2D2

**このパイプラインの特徴:**

- DINOv2のグローバル特徴で初期クラスタリング
- ALIKEDとLightGlueによるローカルマッチングで高精度なポーズ推定

---

## 📌 2) カメラポーズ推定（SfM）

### 📐 Structure-from-Motion（SfM）とは？

SfMは、単なる2D画像から以下を再構築する技術です：

- 各カメラの回転行列 R と並進ベクトル T
- シーンのスパースな3D点群

2D画像のセットのみを使用し、深度情報やカメラポーズなしで、カメラの軌道とシーン構造の両方を復元します。

### 📐 簡易パイプライン

1. キーポイント検出とマッチング（ALIKED + LightGlue）
2. 相対カメラポーズ推定（エッセンシャルマトリクス分解）
3. 三角測量による3D点群推定
4. インクリメンタルに新たな画像をPnPで追加
5. バンドル調整による全体最適化

---

## 🔧 カメラポーズ再構築手順（実践編）

1. ALIKEDで各画像のローカル特徴量を抽出し、`.h5`ファイル保存
2. LightGlueで選択ペアのマッチングを実施
3. h5_to_dbツールでCOLMAP DBにインポート
4. COLMAP内部でマッチング検証＋インクリメンタル再構築（pycolmap経由）

**出力:**

- 各画像に対し回転行列 $R$（3×3）と並進ベクトル $T$（3×1）を取得
- 失敗した場合は NaN をセット

---

## 📦 使用ライブラリまとめ

- **ALIKED**：軽量・高精度な局所特徴量抽出（学習ベース）
- **LightGlue**：トランスフォーマー系特徴マッチャー（頑健な対応推定）
- **COLMAP/pycolmap**：SfM標準エンジン（再構築・マッチング・バンドル調整一式）
  
---

## 📸 カメラポーズの再構築（SfM）
COLMAP、pycolmap、または他のStructure-from-Motion（SfM）エンジンを使用して、以下の手順でカメラポーズの再構築を行います。​

###  🔧 実際の手順
1. ALIKEDによるローカル特徴の抽出
各画像に対して、ALIKEDを用いてローカル特徴（キーポイントとディスクリプタ）を抽出し、.h5ファイルに保存します。

2. LightGlueによる特徴マッチング
選択された画像ペア（グローバル類似度で絞り込むか、全組み合わせ）に対して、LightGlueを使用して特徴のマッチングを行います。

3. COLMAPデータベースへのインポート
h5_to_dbユーティリティを使用して、抽出した特徴とマッチング結果をCOLMAPのデータベースにインポートします。

4. COLMAPでのマッチングと幾何学的検証
COLMAP（pycolmap API）を使用して、全組み合わせのマッチングと幾何学的検証を実行します。

5. インクリメンタルな再構築
COLMAPを用いて、以下の手順でインクリメンタルな再構築を行います:
- 三角測量
- PnP（Perspective-n-Point）によるカメラポーズ推定
- バンドル調整（Bundle Adjustment）​

###  📤 出力
各画像に対して、以下の情報を取得します:​

- 回転行列 $R$（3×3）

- 並進ベクトル $T$（3×1）​

再構築に失敗した画像（例えば、マッチングが不十分な場合）には、ポーズフィールドに NaN を割り当てます。

---​

## 🧠 まとめ
このパイプラインでは、以下のようにグローバル特徴とローカル特徴を組み合わせて使用します:​

- グローバル特徴（DINOv2）: 画像のクラスタリングに使用し、シーンのグループ化を行います。

- ローカル特徴（ALIKED）と特徴マッチング（LightGlue）: 各シーン内での詳細なカメラポーズの再構築に使用します。​

このアプローチにより、画像を正確にクラスタリングし、各クラスタ内で高精度な3D再構築を実現することが可能です。

---
