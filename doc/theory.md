
# 🧠 Image Matchingに必要な理論

## 📂 データセット構造
- 各データセットは複数のシーンで構成されています。
- シーンとは、同じオブジェクトの異なる視点など、関連する画像のグループです。
- 一部の画像は外れ値（アウトライア）であり、どのシーンにも属しません。

**テストデータでのタスク:**

1. 画像をシーンを表すグループにクラスタリングする。
2. 各画像をクラスタに割り当てるか、外れ値としてラベル付けする。
3. 各画像（外れ値を除く）のカメラポーズ（R, T）を再構築する。

---

## 🧠 カメラポーズ推定とmAA

- カメラポーズは、回転行列（3×3）と並進ベクトル（3D）で表されます。
- トレーニングデータでは、真のカメラポーズ（グラウンドトゥルース）が提供されます。
- テストデータでは、これを予測する必要があります。

**カメラ中心Cの計算:**

$$
C = -R^T \cdot T
$$

これは、カメラポーズを3D空間の点に変換します。

**Structure from Motion (SfM):**

- 各画像のカメラポーズを再構築します。
- シーンのスパースな3D点群を生成します。

**予測されたカメラ中心の比較:**

- 予測されたカメラ中心Cとグラウンドトゥルースの中心Cgを比較するために、類似変換T（スケール、回転、並進）を適用します。
- これは、予測された3Dカメラ位置をグラウンドトゥルースに整列させます。

**類似変換の計算:**

- 3つの非共線な点の対応（トリプレット）を使用します。
- 各トリプレットには、予測された3D位置Ciと既知のグラウンドトゥルース位置Cgiが含まれます。

**RANSACアルゴリズム:**

- 多数のトリプレットをランダムにテストし、外れ値を無視して最適な変換を見つけます。

**Hornの方法:**

- 2つの3D点群間の最適な類似変換（スケールs、回転R、並進t）を計算します。

$$
T(C_i) = s \cdot R \cdot C_i + t
$$

**登録カメラの判定:**

$$
	ext{is\_registered} = (\| T(C_i) - Cg_i \| < 	ext{threshold})
$$

ここで、 $\| \cdot \|$ は2つの3D点間のユークリッド距離を表します。

**mAA（mean Average Accuracy）の計算:**

1. 各シーンに対して、複数の閾値 $t_j$ で登録カメラの割合 $r_j$ を計算します。
2. 各シーンの平均精度AAを求めます。
3. すべてのシーンのAAを平均して、データセット全体のmAAを計算します。

---

## 🧠 クラスタリングと最終評価スコア

**評価プロセス:**

- 予測されたクラスタ $C_{kj}$ とグラウンドトゥルースのシーン $S_{ki}$ を比較します。
- 各シーン $S_{ki}$ に対して、最も高いmAAを与えるクラスタ $C_{kji}$ を選択します。
- 外れ値としてラベル付けされた画像は、このマッチングステップから除外されます。

**クラスタリングスコア（精度）の計算:**

$$
	ext{clustering\_score} = rac{|S \cap C|}{|C|}
$$

ここで、 $|S \cap C|$ は予測されたクラスタと実際のシーンの重複する画像の数、 $|C|$ は予測されたクラスタ内の画像の総数です。

**ポーズスコア（mAA）の計算:**

- 各画像の予測されたカメラ中心をグラウンドトゥルースに整列させます。
- 各閾値で登録カメラの数をカウントし、登録率を計算します。
- 各シーンの登録率を平均してmAAを求めます。
- すべてのシーンのmAAを平均して、データセット全体のmAAを計算します。

**データセットごとの最終スコア:**

$$
	ext{final\_score\_per\_dataset} = rac{2 \cdot (	ext{mAA} \cdot 	ext{clustering\_score})}{	ext{mAA} + 	ext{clustering\_score}}
$$

**最終チャレンジスコア:**

$$
	ext{mean}(	ext{final\_score\_per\_dataset})
$$

---

## 🧩 シーンクラスタリングとポーズ推定（SfM）

### 📌 1) シーンクラスタリング

#### 📘 グラフベースのクラスタリング（続き）

**パイプラインでの使用:**

1. DINOv2で抽出した画像埋め込み間のコサイン距離を計算します。
2. 以下の条件でグラフを構築します:
   - 各ノードは画像を表します。
   - 距離が閾値（例: 0.3）未満ならエッジを作成します。
3. このグラフの連結成分をそれぞれ予測されたシーンとみなします。
4. エッジを持たない孤立ノードは外れ値として扱います。

**注意:**  
この段階ではALIKEDやLightGlueは使わず、これらは後段の局所特徴抽出・マッチング時に使用します。

---

## 🔍 ローカルキーポイントとディスクリプタ（一般理論）

- キーポイントとは視点や照明の変化に安定な、特徴的な画像領域（コーナー、エッジ、ブロブなど）です。
- ローカルディスクリプタはキーポイント周辺を記述する特徴ベクトルであり、異なる画像間で対応付けを行います。

**代表的な手法:**

- 手工設計: SIFT, ORB, AKAZE
- 学習ベース: SuperPoint, D2Net, R2D2

**このパイプラインの特徴:**

- DINOv2のグローバル特徴で初期クラスタリング
- ALIKEDとLightGlueによるローカルマッチングで高精度なポーズ推定

---

## 📌 2) カメラポーズ推定（SfM）

### 📐 Structure-from-Motion（SfM）とは？

SfMは、単なる2D画像から以下を再構築する技術です：

- 各カメラの回転行列 R と並進ベクトル T
- シーンのスパースな3D点群

### 📐 簡易パイプライン

1. キーポイント検出とマッチング（ALIKED + LightGlue）
2. 相対カメラポーズ推定（エッセンシャルマトリクス分解）
3. 三角測量による3D点群推定
4. インクリメンタルに新たな画像をPnPで追加
5. バンドル調整による全体最適化

---

## 🔧 カメラポーズ再構築手順（実践編）

1. ALIKEDで各画像のローカル特徴量を抽出し、`.h5`ファイル保存
2. LightGlueで選択ペアのマッチングを実施
3. h5_to_dbツールでCOLMAP DBにインポート
4. COLMAP内部でマッチング検証＋インクリメンタル再構築（pycolmap経由）

**出力:**

- 各画像に対し回転行列 $R$（3×3）と並進ベクトル $T$（3×1）を取得
- 失敗した場合は NaN をセット

---

## 📦 使用ライブラリまとめ

- **ALIKED**：軽量・高精度な局所特徴量抽出（学習ベース）
- **LightGlue**：トランスフォーマー系特徴マッチャー（頑健な対応推定）
- **COLMAP/pycolmap**：SfM標準エンジン（再構築・マッチング・バンドル調整一式）

---
