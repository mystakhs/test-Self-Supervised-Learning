{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4aec3df",
   "metadata": {},
   "source": [
    "# Baselineè§£èª¬ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ï¼šDINOv2 + ALIKED + LightGlueã«ã‚ˆã‚‹Image Matching Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d23d6e",
   "metadata": {},
   "source": [
    "### â‘  ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ãƒ»åˆæœŸè¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b746cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚³ãƒ³ãƒšç’°å¢ƒã§å‹•ä½œã•ã›ã‚‹ãŸã‚ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸è¦ãªå½¢ã§ä¾å­˜é–¢ä¿‚ã¨é‡ã¿ã‚’èª­ã¿è¾¼ã¿\n",
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33859c20",
   "metadata": {},
   "source": [
    "**ä¸Šè¨˜ã‚³ãƒ¼ãƒ‰ã¯ã€ALIKEDã¨LightGlueã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã«é…ç½®ã™ã‚‹å‰å‡¦ç†ã€‚**\n",
    "\n",
    "Kaggleæå‡ºæ™‚ã¯ãƒãƒƒãƒˆæ¥ç¶šã§ããªã„ãŸã‚ã€å¿…é ˆã‚¹ãƒ†ãƒƒãƒ—ã€‚\n",
    "\n",
    "- LightGlueã€Korniaãªã©ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆãªã—ã§ã‚‚å‹•ãã‚ˆã†ã«ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€‚\n",
    "\n",
    "- ALIKEDã¨LightGlueã®å­¦ç¿’æ¸ˆã¿é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼ã€‚\n",
    "\n",
    "- ã“ã‚Œã«ã‚ˆã‚Šã€ã‚³ãƒ³ãƒšç’°å¢ƒï¼ˆå¤–éƒ¨ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ã‚ã‚Šï¼‰ã§ã‚‚æ¨è«–ã§ãã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b541b",
   "metadata": {},
   "source": [
    "### â‘¡ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨åŸºæœ¬ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc643a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "from lightglue import match_pair, ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2532805",
   "metadata": {},
   "source": [
    "**åŸºæœ¬çš„ãªä¾å­˜é–¢ä¿‚ã¨ã€Kaggleæä¾›ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆ`database.py`ã‚„`metric.py`ï¼‰ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚**\n",
    "\n",
    "- Korniaï¼šç”»åƒèª­ã¿è¾¼ã¿ãƒ»å‰å‡¦ç†ï¼ˆPyTorchãƒ™ãƒ¼ã‚¹ï¼‰\n",
    "\n",
    "- LightGlueã€ALIKEDï¼šå±€æ‰€ç‰¹å¾´ç‚¹æ¤œå‡ºï¼†ãƒãƒƒãƒãƒ³ã‚°\n",
    "\n",
    "- Transformersï¼šDINOv2ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã™ãŸã‚\n",
    "\n",
    "- pycolmapï¼šStructure-from-Motionï¼ˆSfMï¼‰å®Ÿè¡Œ\n",
    "\n",
    "- metricï¼šã‚³ãƒ³ãƒšå…¬å¼ã‚¹ã‚³ã‚¢è¨ˆç®—ç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£\n",
    "\n",
    "- h5_to_dbã€databaseï¼šç‰¹å¾´ç‚¹ã¨ãƒãƒƒãƒã‚’COLMAP DBã«ç™»éŒ²ã™ã‚‹ãƒ„ãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901bb95e",
   "metadata": {},
   "source": [
    "### â‘¢ ãƒ‡ãƒã‚¤ã‚¹è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17538bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc510c",
   "metadata": {},
   "source": [
    "### â‘£ ç”»åƒèª­ã¿è¾¼ã¿è£œåŠ©é–¢æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f06c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a526057",
   "metadata": {},
   "source": [
    "ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’Tensorå½¢å¼ï¼ˆ[B,C,H,W]ï¼‰ã§èª­ã¿è¾¼ã‚€ã€‚RGBã‹ã¤32bitç²¾åº¦ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff745e",
   "metadata": {},
   "source": [
    "## ğŸŒ ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´æŠ½å‡ºãƒ»ç”»åƒãƒšã‚¢é¸å®šå‡¦ç†\n",
    "### â‘¤DINOv2ã«ã‚ˆã‚‹ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´æŠ½å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must Use efficientnet global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, device = torch.device('cpu')):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        timg = load_torch_image(img_fname_full)\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_mac.detach().cpu())\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "    return global_descs_dinov2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19208a95",
   "metadata": {},
   "source": [
    "DINOv2ã‚’ä½¿ã£ã¦å„ç”»åƒã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´é‡ï¼ˆMACç‰¹å¾´ï¼‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚\n",
    "\n",
    "ç‰¹å¾´ã‚’L2æ­£è¦åŒ–ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c6584",
   "metadata": {},
   "source": [
    "### â‘¥ ç”»åƒãƒšã‚¢ç”Ÿæˆï¼ˆå…¨æ¢ç´¢ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da56c0cf",
   "metadata": {},
   "source": [
    "å…¨ç”»åƒçµ„ã¿åˆã‚ã›ã®ãƒšã‚¢ã‚’ä½œã‚‹ï¼ˆå®Œå…¨ã‚°ãƒªãƒƒãƒ‰ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d3c355",
   "metadata": {},
   "source": [
    "### â‘¦ ç”»åƒãƒšã‚¢ç”Ÿæˆï¼ˆã‚·ãƒ§ãƒ¼ãƒˆãƒªã‚¹ãƒˆåŒ–ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffabb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 30,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "    descs = get_global_desc(fnames, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "    # removing half\n",
    "    mask = dm <= sim_th\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < 1000:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "                total+=1\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    return matching_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d61be8",
   "metadata": {},
   "source": [
    "DINOv2ç‰¹å¾´é–“ã®ãƒšã‚¢ãƒ¯ã‚¤ã‚ºè·é›¢ã‚’è¨ˆç®—ã€‚\n",
    "\n",
    "é¡ä¼¼åº¦ãŒé«˜ã„ç”»åƒåŒå£«ã ã‘ãƒšã‚¢ã«ã™ã‚‹ï¼ˆã‚¹ãƒ‘ãƒ¼ã‚¹ãªãƒãƒƒãƒå€™è£œï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c5ff99",
   "metadata": {},
   "source": [
    "## ğŸ“‚ ãƒ­ãƒ¼ã‚«ãƒ«ç‰¹å¾´æŠ½å‡ºã¨ãƒãƒƒãƒãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64edda",
   "metadata": {},
   "source": [
    "### â‘§ ALIKEDã«ã‚ˆã‚‹ãƒ­ãƒ¼ã‚«ãƒ«ç‰¹å¾´æŠ½å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c491757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_aliked(img_fnames,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 1024,\n",
    "                  device=torch.device('cpu')):\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.01, resize=resize_to).eval().to(device, dtype)\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n",
    "        for img_path in tqdm(img_fnames):\n",
    "            img_fname = img_path.split('/')[-1]\n",
    "            key = img_fname\n",
    "            with torch.inference_mode():\n",
    "                image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "                feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None\n",
    "                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n",
    "                descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n",
    "                f_kp[key] = kpts\n",
    "                f_desc[key] = descs\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1801bdbd",
   "metadata": {},
   "source": [
    "ALIKEDã§æŠ½å‡ºã—ãŸç‰¹å¾´ç‚¹ã¨ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚¿ã‚’h5å½¢å¼ã§ä¿å­˜ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c1455",
   "metadata": {},
   "source": [
    "###  â‘¨LightGlueã«ã‚ˆã‚‹ãƒãƒƒãƒãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7df92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_with_lightglue(img_fnames,\n",
    "                   index_pairs,\n",
    "                   feature_dir = '.featureout',\n",
    "                   device=torch.device('cpu'),\n",
    "                   min_matches=25,verbose=True):\n",
    "    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n",
    "                                                \"depth_confidence\": -1,\n",
    "                                                 \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n",
    "        h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n",
    "        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n",
    "            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n",
    "            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n",
    "            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                         desc2,\n",
    "                                         KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                                         KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            n_matches = len(idxs)\n",
    "            if verbose:\n",
    "                print (f'{key1}-{key2}: {n_matches} matches')\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                 group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a965f6",
   "metadata": {},
   "source": [
    "LightGlueã§ç‰¹å¾´é‡ã‚’å¯¾å¿œä»˜ã‘ã€æœ€ä½é™ã®ãƒãƒƒãƒæ•°ã‚’æº€ãŸã™ã‚‚ã®ã‚’ä¿å­˜ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0172d6",
   "metadata": {},
   "source": [
    "## ğŸ“‚ Structure-from-Motion (COLMAP reconstruct)\n",
    "### â‘© COLMAPãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f20d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b0cae2",
   "metadata": {},
   "source": [
    "äº‹å‰ã«æŠ½å‡ºãƒ»ãƒãƒƒãƒãƒ³ã‚°ã—ãŸç‰¹å¾´ç‚¹æƒ…å ±ã‚’COLMAPå½¢å¼ã®DBã«æ ¼ç´ã€‚\n",
    "\n",
    "ï¼ˆâ€»ã“ã®å¾Œã€pycolmapã§match_exhaustive â†’ incremental_mappingã‚’å®Ÿè¡Œã€æœ€çµ‚çš„ã«ã‚«ãƒ¡ãƒ©ãƒãƒ¼ã‚º(R, T)ã‚’æ¨å®šã—ã¦ã„ã¾ã™ã€‚ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb586ee",
   "metadata": {},
   "source": [
    "## ğŸ” ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã”ã¨ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†ãƒ«ãƒ¼ãƒ—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c904be",
   "metadata": {},
   "source": [
    "### â‘ª ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨Predictionã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå®šç¾©\n",
    "1ç”»åƒã”ã¨ã®äºˆæ¸¬çµæœã‚’ä¿æŒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã€‚\n",
    "\n",
    "cluster_indexï¼ˆã‚¯ãƒ©ã‚¹ã‚¿IDï¼‰ã€rotationï¼ˆå›è»¢è¡Œåˆ—Rï¼‰ã€translationï¼ˆä¸¦é€²ãƒ™ã‚¯ãƒˆãƒ«Tï¼‰ãªã©ã‚’æ ¼ç´ã™ã‚‹ãŸã‚ã«ä½¿ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c70deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39576a7",
   "metadata": {},
   "source": [
    "### â‘« sample_submissionèª­ã¿è¾¼ã¿ã¨åˆæœŸãƒ‡ãƒ¼ã‚¿æ•´å½¢\n",
    "sample_submission.csvï¼ˆãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆï¼‰ã‹train_labels.csvï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆï¼‰ã‚’èª­ã¿è¾¼ã‚€ã€‚\n",
    "\n",
    "å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã”ã¨ã«Predictionã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒªã‚¹ãƒˆã‚’ä½œæˆã—ã¦samplesè¾æ›¸ã«æ ¼ç´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15821c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = False  # æå‡ºç”¨ï¼šFalseï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãªã‚‰Trueï¼‰\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/kaggle/working/result/'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "samples = {}\n",
    "for _, row in competition_data.iterrows():\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a3e567",
   "metadata": {},
   "source": [
    "### â‘¬ å‡¦ç†ãƒ•ãƒ©ã‚°ã¨ã‚¿ã‚¤ãƒŸãƒ³ã‚°æ¸¬å®šã®æº–å‚™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8437ae",
   "metadata": {},
   "source": [
    "ã‚¬ãƒ¼ãƒ™ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆãƒ¡ãƒ¢ãƒªæ•´ç†ï¼‰å®Ÿè¡Œã€‚\n",
    "\n",
    "ãƒ‡ãƒãƒƒã‚°ç”¨ã«ç”»åƒæšæ•°åˆ¶é™ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¶é™ã‚‚å¯èƒ½ã€‚\n",
    "\n",
    "å„ã‚¹ãƒ†ãƒƒãƒ—ã®æ™‚é–“è¨ˆæ¸¬ã®ãŸã‚ã®è¾æ›¸ã‚’ç”¨æ„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d00eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "if is_train:\n",
    "    # max_images = 5\n",
    "\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "    \t# New data.\n",
    "    \t'amy_gardens',\n",
    "    \t'ETs',\n",
    "    \t'fbk_vineyard',\n",
    "    \t'stairs',\n",
    "    \t# Data from IMC 2023 and 2024.\n",
    "    \t# 'imc2024_dioscuri_baalshamin',\n",
    "    \t# 'imc2023_theather_imc2024_church',\n",
    "    \t# 'imc2023_heritage',\n",
    "    \t# 'imc2023_haiper',\n",
    "    \t# 'imc2024_lizard_pond',\n",
    "    \t# Crowdsourced PhotoTourism data.\n",
    "    \t# 'pt_stpeters_stpauls',\n",
    "    \t# 'pt_brandenburg_british_buckingham',\n",
    "    \t# 'pt_piazzasanmarco_grandplace',\n",
    "    \t# 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "timings = {\n",
    "    \"shortlisting\": [],\n",
    "    \"feature_detection\": [],\n",
    "    \"feature_matching\": [],\n",
    "    \"RANSAC\": [],\n",
    "    \"Reconstruction\": [],\n",
    "}\n",
    "mapping_result_strs = []\n",
    "\n",
    "print(f\"Extracting on device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f99f170",
   "metadata": {},
   "source": [
    "### â‘­ forãƒ«ãƒ¼ãƒ—æœ¬ä½“ï¼ˆå„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦å‡¦ç†ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303d36ec",
   "metadata": {},
   "source": [
    "1. é¡ä¼¼ç”»åƒã®ã‚·ãƒ§ãƒ¼ãƒˆãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "\n",
    "2. ALIKEDã§å±€æ‰€ç‰¹å¾´ã‚’æŠ½å‡º\n",
    "\n",
    "3. LightGlueã§ç‰¹å¾´ãƒãƒƒãƒãƒ³ã‚°\n",
    "\n",
    "4. COLMAPã§ãƒãƒƒãƒæƒ…å ±ã‚’DBç™»éŒ²ï¼‹RANSACæ¤œè¨¼\n",
    "\n",
    "5. SfMã«ã‚ˆã‚‹å†æ§‹æˆï¼ˆã‚«ãƒ¡ãƒ©ãƒãƒ¼ã‚ºæ¨å®šï¼‰\n",
    "\n",
    "6. æˆåŠŸã™ã‚Œã°ã€å„ç”»åƒã”ã¨ã«ã‚¯ãƒ©ã‚¹ã‚¿ç•ªå·ãƒ»å›è»¢è¡Œåˆ—ãƒ»ä¸¦é€²ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä¿å­˜\n",
    "\n",
    "ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ä¾‹å¤–ã‚’ã‚­ãƒ£ãƒƒãƒã—ã¦ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ï¼ˆå£Šã‚ŒãŸã‚·ãƒ¼ãƒ³ã§ã‚‚æ­¢ã¾ã‚‰ãªã„ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67925a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, predictions in samples.items():\n",
    "    if datasets_to_process and dataset not in datasets_to_process:\n",
    "        print(f'Skipping \"{dataset}\"')\n",
    "        continue\n",
    "    \n",
    "    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "    images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "    if max_images is not None:\n",
    "        images = images[:max_images]\n",
    "\n",
    "    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "\n",
    "    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "\n",
    "    feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # (1) é¡ä¼¼ç”»åƒãƒšã‚¢é¸å®š\n",
    "        t = time()\n",
    "        index_pairs = get_image_pairs_shortlist(\n",
    "            images,\n",
    "            sim_th = 0.3, # should be strict\n",
    "            min_pairs = 20, # we should select at least min_pairs PER IMAGE with biggest similarity\n",
    "            exhaustive_if_less = 20,\n",
    "            device=device\n",
    "        )\n",
    "        timings['shortlisting'].append(time() - t)\n",
    "        print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "        gc.collect()\n",
    "\n",
    "        # (2) å±€æ‰€ç‰¹å¾´æŠ½å‡º\n",
    "        t = time()\n",
    "        detect_aliked(images, feature_dir, 4096, device=device)\n",
    "        gc.collect()\n",
    "        timings['feature_detection'].append(time() - t)\n",
    "        print(f'Features detected in {time() - t:.4f} sec')\n",
    "\n",
    "        # (3) ãƒãƒƒãƒãƒ³ã‚°\n",
    "        t = time()\n",
    "        match_with_lightglue(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n",
    "        timings['feature_matching'].append(time() - t)\n",
    "        print(f'Features matched in {time() - t:.4f} sec')\n",
    "\n",
    "        # (4) COLMAP DBä½œæˆã¨RANSAC\n",
    "        database_path = os.path.join(feature_dir, 'colmap.db')\n",
    "        if os.path.isfile(database_path):\n",
    "            os.remove(database_path)\n",
    "        gc.collect()\n",
    "        sleep(1)\n",
    "        import_into_colmap(images_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "        output_path = f'{feature_dir}/colmap_rec_aliked'\n",
    "        \n",
    "        t = time()\n",
    "        pycolmap.match_exhaustive(database_path)\n",
    "        timings['RANSAC'].append(time() - t)\n",
    "        print(f'Ran RANSAC in {time() - t:.4f} sec')\n",
    "\n",
    "        # (5) Structure-from-Motionå†æ§‹æˆ\n",
    "        # By default colmap does not generate a reconstruction if less than 10 images are registered.\n",
    "        # Lower it to 3.\n",
    "        mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "        mapper_options.min_model_size = 3\n",
    "        mapper_options.max_num_models = 25\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        t = time()\n",
    "        maps = pycolmap.incremental_mapping(\n",
    "            database_path=database_path, \n",
    "            image_path=images_dir,\n",
    "            output_path=output_path,\n",
    "            options=mapper_options)\n",
    "        sleep(1)\n",
    "        timings['Reconstruction'].append(time() - t)\n",
    "        print(f'Reconstruction done in  {time() - t:.4f} sec')\n",
    "        print(maps)\n",
    "\n",
    "        clear_output(wait=False)\n",
    "\n",
    "        # (6) äºˆæ¸¬çµæœã‚’Predictionã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«åæ˜ \n",
    "        registered = 0\n",
    "        for map_index, cur_map in maps.items():\n",
    "            for index, image in cur_map.images.items():\n",
    "                prediction_index = filename_to_index[image.name]\n",
    "                predictions[prediction_index].cluster_index = map_index\n",
    "                predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n",
    "                predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n",
    "                registered += 1\n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images with {len(maps)} clusters'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # raise e\n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "\n",
    "print('\\nResults')\n",
    "for s in mapping_result_strs:\n",
    "    print(s)\n",
    "\n",
    "print('\\nTimings')\n",
    "for k, v in timings.items():\n",
    "    print(f'{k} -> total={sum(v):.02f} sec.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a468",
   "metadata": {},
   "source": [
    "## ğŸ“„ Submissionãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "\n",
    "!head {submission_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c086f99",
   "metadata": {},
   "source": [
    "æœ€çµ‚çµæœã‚’`submission.csv`ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d037cc",
   "metadata": {},
   "source": [
    "## ğŸ§® ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆis_train=Trueæ™‚ã®ã¿ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6593a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_train:\n",
    "    t = time()\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f'Computed metric in: {time() - t:.02f} sec.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc1ac4",
   "metadata": {},
   "source": [
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã§å®Ÿè¡Œã™ã‚‹å ´åˆã®ã¿ã€æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦å…¬å¼è©•ä¾¡æŒ‡æ¨™ï¼ˆmAA + ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç²¾åº¦ï¼‰ã‚’ç®—å‡ºã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112d305",
   "metadata": {},
   "source": [
    "## âœ… ã¾ã¨ã‚\n",
    "1. ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´ï¼ˆDINOv2ï¼‰ã§é¡ä¼¼ç”»åƒé¸å®š\n",
    "\n",
    "2. å±€æ‰€ç‰¹å¾´ï¼ˆALIKED+LightGlueï¼‰ã§ãƒãƒƒãƒãƒ³ã‚°\n",
    "\n",
    "3. Structure-from-Motionï¼ˆCOLMAPï¼‰ã§ã‚«ãƒ¡ãƒ©ãƒãƒ¼ã‚ºæ¨å®š\n",
    "\n",
    "4. çµæœã‚’submissionå½¢å¼ã§ä¿å­˜\n",
    "\n",
    "ã¨ã„ã†ä¸€è²«ã—ãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’çµ„ã‚“ã§ã„ã‚‹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6eda2d",
   "metadata": {},
   "source": [
    "## ğŸ—ºï¸ å…¨ä½“å‡¦ç†ã®ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆï¼ˆMermaidè¨˜æ³•ã«ã‚ˆã‚‹å›³è§£ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5693ad11",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    A[ã‚¹ã‚¿ãƒ¼ãƒˆ] --> B[ä¾å­˜é–¢ä¿‚ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ãƒ¢ãƒ‡ãƒ«æº–å‚™]\n",
    "    B --> C[ç”»åƒã®èª­ã¿è¾¼ã¿ã¨å‰å‡¦ç†]\n",
    "    C --> D[DINOv2ã§ã‚°ãƒ­ãƒ¼ãƒãƒ«ç‰¹å¾´æŠ½å‡º]\n",
    "    D --> E[é¡ä¼¼åº¦ã«åŸºã¥ãç”»åƒãƒšã‚¢ã®ã‚·ãƒ§ãƒ¼ãƒˆãƒªã‚¹ãƒˆ]\n",
    "    E --> F[ALIKEDã§å±€æ‰€ç‰¹å¾´æŠ½å‡º]\n",
    "    F --> G[LightGlueã§ç‰¹å¾´ãƒãƒƒãƒãƒ³ã‚°]\n",
    "    G --> H[COLMAPç”¨DBã«ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ]\n",
    "    H --> I[COLMAPã«ã‚ˆã‚‹RANSACã¨ãƒãƒƒãƒ”ãƒ³ã‚°]\n",
    "    I --> J[ã‚«ãƒ¡ãƒ©å§¿å‹¢ï¼ˆR, Tï¼‰ã®æ¨å®š]\n",
    "    J --> K[ã‚¯ãƒ©ã‚¹ã‚¿å‰²å½“ã¨çµæœã®ä¿å­˜]\n",
    "    K --> L[submission.csv ã®ç”Ÿæˆ]\n",
    "    L --> M[ã‚¹ã‚³ã‚¢ç®—å‡ºï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã®ã¿ï¼‰]\n",
    "    M --> Z[çµ‚äº†]\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
