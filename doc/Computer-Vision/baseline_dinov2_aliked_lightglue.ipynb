{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4aec3df",
   "metadata": {},
   "source": [
    "# Baseline解説ノートブック：DINOv2 + ALIKED + LightGlueによるImage Matching Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d23d6e",
   "metadata": {},
   "source": [
    "### ① 依存関係のインストール・初期設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b746cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# コンペ環境で動作させるため、インターネット不要な形で依存関係と重みを読み込み\n",
    "!pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/aliked-n16.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33859c20",
   "metadata": {},
   "source": [
    "**上記コードは、ALIKEDとLightGlueの学習済みモデルをローカル環境に配置する前処理。**\n",
    "\n",
    "Kaggle提出時はネット接続できないため、必須ステップ。\n",
    "\n",
    "- LightGlue、Korniaなど、インターネットなしでも動くようにローカルパッケージをインストール。\n",
    "\n",
    "- ALIKEDとLightGlueの学習済み重みファイルをキャッシュディレクトリにコピー。\n",
    "\n",
    "- これにより、コンペ環境（外部アクセス制限あり）でも推論できる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b541b",
   "metadata": {},
   "source": [
    "### ② ライブラリのインポートと基本ユーティリティ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc643a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time, sleep\n",
    "import gc\n",
    "import numpy as np\n",
    "import h5py\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "\n",
    "from lightglue import match_pair, ALIKED, LightGlue\n",
    "from lightglue.utils import load_image, rbd\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "import pycolmap\n",
    "sys.path.append('/kaggle/input/imc25-utils')\n",
    "from database import *\n",
    "from h5_to_db import *\n",
    "import metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2532805",
   "metadata": {},
   "source": [
    "**基本的な依存関係と、Kaggle提供のユーティリティ（`database.py`や`metric.py`）をインポートします。**\n",
    "\n",
    "- Kornia：画像読み込み・前処理（PyTorchベース）\n",
    "\n",
    "- LightGlue、ALIKED：局所特徴点検出＆マッチング\n",
    "\n",
    "- Transformers：DINOv2モデルを呼び出すため\n",
    "\n",
    "- pycolmap：Structure-from-Motion（SfM）実行\n",
    "\n",
    "- metric：コンペ公式スコア計算用ユーティリティ\n",
    "\n",
    "- h5_to_db、database：特徴点とマッチをCOLMAP DBに登録するツール"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901bb95e",
   "metadata": {},
   "source": [
    "### ③ デバイス設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17538bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = K.utils.get_cuda_device_if_available(0)\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc510c",
   "metadata": {},
   "source": [
    "### ④ 画像読み込み補助関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f06c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a526057",
   "metadata": {},
   "source": [
    "画像ファイルをTensor形式（[B,C,H,W]）で読み込む。RGBかつ32bit精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ff745e",
   "metadata": {},
   "source": [
    "## 🌐 グローバル特徴抽出・画像ペア選定処理\n",
    "### ⑤DINOv2によるグローバル特徴抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must Use efficientnet global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, device = torch.device('cpu')):\n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1')\n",
    "    model = model.eval()\n",
    "    model = model.to(device)\n",
    "    global_descs_dinov2 = []\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        timg = load_torch_image(img_fname_full)\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=timg, return_tensors=\"pt\", do_rescale=False).to(device)\n",
    "            outputs = model(**inputs)\n",
    "            dino_mac = F.normalize(outputs.last_hidden_state[:,1:].max(dim=1)[0], dim=1, p=2)\n",
    "        global_descs_dinov2.append(dino_mac.detach().cpu())\n",
    "    global_descs_dinov2 = torch.cat(global_descs_dinov2, dim=0)\n",
    "    return global_descs_dinov2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19208a95",
   "metadata": {},
   "source": [
    "DINOv2を使って各画像のグローバル特徴量（MAC特徴）を抽出します。\n",
    "\n",
    "特徴をL2正規化してベクトル化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c6584",
   "metadata": {},
   "source": [
    "### ⑥ 画像ペア生成（全探索）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_pairs_exhaustive(img_fnames):\n",
    "    index_pairs = []\n",
    "    for i in range(len(img_fnames)):\n",
    "        for j in range(i+1, len(img_fnames)):\n",
    "            index_pairs.append((i,j))\n",
    "    return index_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da56c0cf",
   "metadata": {},
   "source": [
    "全画像組み合わせのペアを作る（完全グリッド）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d3c355",
   "metadata": {},
   "source": [
    "### ⑦ 画像ペア生成（ショートリスト化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffabb70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 30,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames)\n",
    "    descs = get_global_desc(fnames, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "    # removing half\n",
    "    mask = dm <= sim_th\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < 1000:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "                total+=1\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    return matching_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d61be8",
   "metadata": {},
   "source": [
    "DINOv2特徴間のペアワイズ距離を計算。\n",
    "\n",
    "類似度が高い画像同士だけペアにする（スパースなマッチ候補）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c5ff99",
   "metadata": {},
   "source": [
    "## 📂 ローカル特徴抽出とマッチング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a64edda",
   "metadata": {},
   "source": [
    "### ⑧ ALIKEDによるローカル特徴抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c491757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_aliked(img_fnames,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 1024,\n",
    "                  device=torch.device('cpu')):\n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    extractor = ALIKED(max_num_keypoints=num_features, detection_threshold=0.01, resize=resize_to).eval().to(device, dtype)\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp, \\\n",
    "         h5py.File(f'{feature_dir}/descriptors.h5', mode='w') as f_desc:\n",
    "        for img_path in tqdm(img_fnames):\n",
    "            img_fname = img_path.split('/')[-1]\n",
    "            key = img_fname\n",
    "            with torch.inference_mode():\n",
    "                image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "                feats0 = extractor.extract(image0)  # auto-resize the image, disable with resize=None\n",
    "                kpts = feats0['keypoints'].reshape(-1, 2).detach().cpu().numpy()\n",
    "                descs = feats0['descriptors'].reshape(len(kpts), -1).detach().cpu().numpy()\n",
    "                f_kp[key] = kpts\n",
    "                f_desc[key] = descs\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1801bdbd",
   "metadata": {},
   "source": [
    "ALIKEDで抽出した特徴点とディスクリプタをh5形式で保存します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c1455",
   "metadata": {},
   "source": [
    "###  ⑨LightGlueによるマッチング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7df92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_with_lightglue(img_fnames,\n",
    "                   index_pairs,\n",
    "                   feature_dir = '.featureout',\n",
    "                   device=torch.device('cpu'),\n",
    "                   min_matches=25,verbose=True):\n",
    "    lg_matcher = KF.LightGlueMatcher(\"aliked\", {\"width_confidence\": -1,\n",
    "                                                \"depth_confidence\": -1,\n",
    "                                                 \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='r') as f_kp, \\\n",
    "        h5py.File(f'{feature_dir}/descriptors.h5', mode='r') as f_desc, \\\n",
    "        h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            kp1 = torch.from_numpy(f_kp[key1][...]).to(device)\n",
    "            kp2 = torch.from_numpy(f_kp[key2][...]).to(device)\n",
    "            desc1 = torch.from_numpy(f_desc[key1][...]).to(device)\n",
    "            desc2 = torch.from_numpy(f_desc[key2][...]).to(device)\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                         desc2,\n",
    "                                         KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                                         KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            n_matches = len(idxs)\n",
    "            if verbose:\n",
    "                print (f'{key1}-{key2}: {n_matches} matches')\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                 group.create_dataset(key2, data=idxs.detach().cpu().numpy().reshape(-1, 2))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a965f6",
   "metadata": {},
   "source": [
    "LightGlueで特徴量を対応付け、最低限のマッチ数を満たすものを保存します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0172d6",
   "metadata": {},
   "source": [
    "## 📂 Structure-from-Motion (COLMAP reconstruct)\n",
    "### ⑩ COLMAPデータベース構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f20d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_into_colmap(img_dir, feature_dir ='.featureout', database_path = 'colmap.db'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, '', 'simple-pinhole', single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b0cae2",
   "metadata": {},
   "source": [
    "事前に抽出・マッチングした特徴点情報をCOLMAP形式のDBに格納。\n",
    "\n",
    "（※この後、pycolmapでmatch_exhaustive → incremental_mappingを実行、最終的にカメラポーズ(R, T)を推定しています。）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb586ee",
   "metadata": {},
   "source": [
    "## 🔁 データセットごとのメイン処理ループ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c904be",
   "metadata": {},
   "source": [
    "### ⑪ データ準備とPredictionオブジェクト定義\n",
    "1画像ごとの予測結果を保持するデータクラス。\n",
    "\n",
    "cluster_index（クラスタID）、rotation（回転行列R）、translation（並進ベクトルT）などを格納するために使う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c70deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class Prediction:\n",
    "    image_id: str | None\n",
    "    dataset: str\n",
    "    filename: str\n",
    "    cluster_index: int | None = None\n",
    "    rotation: np.ndarray | None = None\n",
    "    translation: np.ndarray | None = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39576a7",
   "metadata": {},
   "source": [
    "### ⑫ sample_submission読み込みと初期データ整形\n",
    "sample_submission.csv（テストセット）かtrain_labels.csv（トレーニングセット）を読み込む。\n",
    "\n",
    "各データセットごとにPredictionオブジェクトリストを作成してsamples辞書に格納。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15821c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = False  # 提出用：False（トレーニング用ならTrue）\n",
    "data_dir = '/kaggle/input/image-matching-challenge-2025'\n",
    "workdir = '/kaggle/working/result/'\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "\n",
    "if is_train:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'train_labels.csv')\n",
    "else:\n",
    "    sample_submission_csv = os.path.join(data_dir, 'sample_submission.csv')\n",
    "\n",
    "competition_data = pd.read_csv(sample_submission_csv)\n",
    "samples = {}\n",
    "for _, row in competition_data.iterrows():\n",
    "    if row.dataset not in samples:\n",
    "        samples[row.dataset] = []\n",
    "    samples[row.dataset].append(\n",
    "        Prediction(\n",
    "            image_id=None if is_train else row.image_id,\n",
    "            dataset=row.dataset,\n",
    "            filename=row.image\n",
    "        )\n",
    "    )\n",
    "\n",
    "for dataset in samples:\n",
    "    print(f'Dataset \"{dataset}\" -> num_images={len(samples[dataset])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a3e567",
   "metadata": {},
   "source": [
    "### ⑬ 処理フラグとタイミング測定の準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8437ae",
   "metadata": {},
   "source": [
    "ガーベジコレクション（メモリ整理）実行。\n",
    "\n",
    "デバッグ用に画像枚数制限・データセット制限も可能。\n",
    "\n",
    "各ステップの時間計測のための辞書を用意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d00eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "max_images = None  # Used For debugging only. Set to None to disable.\n",
    "datasets_to_process = None  # Not the best convention, but None means all datasets.\n",
    "\n",
    "if is_train:\n",
    "    # max_images = 5\n",
    "\n",
    "    # Note: When running on the training dataset, the notebook will hit the time limit and die. Use this filter to run on a few specific datasets.\n",
    "    datasets_to_process = [\n",
    "    \t# New data.\n",
    "    \t'amy_gardens',\n",
    "    \t'ETs',\n",
    "    \t'fbk_vineyard',\n",
    "    \t'stairs',\n",
    "    \t# Data from IMC 2023 and 2024.\n",
    "    \t# 'imc2024_dioscuri_baalshamin',\n",
    "    \t# 'imc2023_theather_imc2024_church',\n",
    "    \t# 'imc2023_heritage',\n",
    "    \t# 'imc2023_haiper',\n",
    "    \t# 'imc2024_lizard_pond',\n",
    "    \t# Crowdsourced PhotoTourism data.\n",
    "    \t# 'pt_stpeters_stpauls',\n",
    "    \t# 'pt_brandenburg_british_buckingham',\n",
    "    \t# 'pt_piazzasanmarco_grandplace',\n",
    "    \t# 'pt_sacrecoeur_trevi_tajmahal',\n",
    "    ]\n",
    "\n",
    "timings = {\n",
    "    \"shortlisting\": [],\n",
    "    \"feature_detection\": [],\n",
    "    \"feature_matching\": [],\n",
    "    \"RANSAC\": [],\n",
    "    \"Reconstruction\": [],\n",
    "}\n",
    "mapping_result_strs = []\n",
    "\n",
    "print(f\"Extracting on device {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f99f170",
   "metadata": {},
   "source": [
    "### ⑭ forループ本体（各データセットについて処理）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303d36ec",
   "metadata": {},
   "source": [
    "1. 類似画像のショートリストを作成\n",
    "\n",
    "2. ALIKEDで局所特徴を抽出\n",
    "\n",
    "3. LightGlueで特徴マッチング\n",
    "\n",
    "4. COLMAPでマッチ情報をDB登録＋RANSAC検証\n",
    "\n",
    "5. SfMによる再構成（カメラポーズ推定）\n",
    "\n",
    "6. 成功すれば、各画像ごとにクラスタ番号・回転行列・並進ベクトルを保存\n",
    "\n",
    "エラー時も例外をキャッチしてスキップする（壊れたシーンでも止まらない）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67925a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, predictions in samples.items():\n",
    "    if datasets_to_process and dataset not in datasets_to_process:\n",
    "        print(f'Skipping \"{dataset}\"')\n",
    "        continue\n",
    "    \n",
    "    images_dir = os.path.join(data_dir, 'train' if is_train else 'test', dataset)\n",
    "    images = [os.path.join(images_dir, p.filename) for p in predictions]\n",
    "    if max_images is not None:\n",
    "        images = images[:max_images]\n",
    "\n",
    "    print(f'\\nProcessing dataset \"{dataset}\": {len(images)} images')\n",
    "\n",
    "    filename_to_index = {p.filename: idx for idx, p in enumerate(predictions)}\n",
    "\n",
    "    feature_dir = os.path.join(workdir, 'featureout', dataset)\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # (1) 類似画像ペア選定\n",
    "        t = time()\n",
    "        index_pairs = get_image_pairs_shortlist(\n",
    "            images,\n",
    "            sim_th = 0.3, # should be strict\n",
    "            min_pairs = 20, # we should select at least min_pairs PER IMAGE with biggest similarity\n",
    "            exhaustive_if_less = 20,\n",
    "            device=device\n",
    "        )\n",
    "        timings['shortlisting'].append(time() - t)\n",
    "        print (f'Shortlisting. Number of pairs to match: {len(index_pairs)}. Done in {time() - t:.4f} sec')\n",
    "        gc.collect()\n",
    "\n",
    "        # (2) 局所特徴抽出\n",
    "        t = time()\n",
    "        detect_aliked(images, feature_dir, 4096, device=device)\n",
    "        gc.collect()\n",
    "        timings['feature_detection'].append(time() - t)\n",
    "        print(f'Features detected in {time() - t:.4f} sec')\n",
    "\n",
    "        # (3) マッチング\n",
    "        t = time()\n",
    "        match_with_lightglue(images, index_pairs, feature_dir=feature_dir, device=device, verbose=False)\n",
    "        timings['feature_matching'].append(time() - t)\n",
    "        print(f'Features matched in {time() - t:.4f} sec')\n",
    "\n",
    "        # (4) COLMAP DB作成とRANSAC\n",
    "        database_path = os.path.join(feature_dir, 'colmap.db')\n",
    "        if os.path.isfile(database_path):\n",
    "            os.remove(database_path)\n",
    "        gc.collect()\n",
    "        sleep(1)\n",
    "        import_into_colmap(images_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "        output_path = f'{feature_dir}/colmap_rec_aliked'\n",
    "        \n",
    "        t = time()\n",
    "        pycolmap.match_exhaustive(database_path)\n",
    "        timings['RANSAC'].append(time() - t)\n",
    "        print(f'Ran RANSAC in {time() - t:.4f} sec')\n",
    "\n",
    "        # (5) Structure-from-Motion再構成\n",
    "        # By default colmap does not generate a reconstruction if less than 10 images are registered.\n",
    "        # Lower it to 3.\n",
    "        mapper_options = pycolmap.IncrementalPipelineOptions()\n",
    "        mapper_options.min_model_size = 3\n",
    "        mapper_options.max_num_models = 25\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        t = time()\n",
    "        maps = pycolmap.incremental_mapping(\n",
    "            database_path=database_path, \n",
    "            image_path=images_dir,\n",
    "            output_path=output_path,\n",
    "            options=mapper_options)\n",
    "        sleep(1)\n",
    "        timings['Reconstruction'].append(time() - t)\n",
    "        print(f'Reconstruction done in  {time() - t:.4f} sec')\n",
    "        print(maps)\n",
    "\n",
    "        clear_output(wait=False)\n",
    "\n",
    "        # (6) 予測結果をPredictionオブジェクトに反映\n",
    "        registered = 0\n",
    "        for map_index, cur_map in maps.items():\n",
    "            for index, image in cur_map.images.items():\n",
    "                prediction_index = filename_to_index[image.name]\n",
    "                predictions[prediction_index].cluster_index = map_index\n",
    "                predictions[prediction_index].rotation = deepcopy(image.cam_from_world.rotation.matrix())\n",
    "                predictions[prediction_index].translation = deepcopy(image.cam_from_world.translation)\n",
    "                registered += 1\n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Registered {registered} / {len(images)} images with {len(maps)} clusters'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # raise e\n",
    "        mapping_result_str = f'Dataset \"{dataset}\" -> Failed!'\n",
    "        mapping_result_strs.append(mapping_result_str)\n",
    "        print(mapping_result_str)\n",
    "\n",
    "print('\\nResults')\n",
    "for s in mapping_result_strs:\n",
    "    print(s)\n",
    "\n",
    "print('\\nTimings')\n",
    "for k, v in timings.items():\n",
    "    print(f'{k} -> total={sum(v):.02f} sec.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a468",
   "metadata": {},
   "source": [
    "## 📄 Submissionファイル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5eb545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_to_str = lambda array: ';'.join([f\"{x:.09f}\" for x in array])\n",
    "none_to_str = lambda n: ';'.join(['nan'] * n)\n",
    "\n",
    "submission_file = '/kaggle/working/submission.csv'\n",
    "with open(submission_file, 'w') as f:\n",
    "    if is_train:\n",
    "        f.write('dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "    else:\n",
    "        f.write('image_id,dataset,scene,image,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in samples:\n",
    "            for prediction in samples[dataset]:\n",
    "                cluster_name = 'outliers' if prediction.cluster_index is None else f'cluster{prediction.cluster_index}'\n",
    "                rotation = none_to_str(9) if prediction.rotation is None else array_to_str(prediction.rotation.flatten())\n",
    "                translation = none_to_str(3) if prediction.translation is None else array_to_str(prediction.translation)\n",
    "                f.write(f'{prediction.image_id},{prediction.dataset},{cluster_name},{prediction.filename},{rotation},{translation}\\n')\n",
    "\n",
    "!head {submission_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c086f99",
   "metadata": {},
   "source": [
    "最終結果を`submission.csv`として保存します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d037cc",
   "metadata": {},
   "source": [
    "## 🧮 スコア計算（is_train=True時のみ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6593a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_train:\n",
    "    t = time()\n",
    "    final_score, dataset_scores = metric.score(\n",
    "        gt_csv='/kaggle/input/image-matching-challenge-2025/train_labels.csv',\n",
    "        user_csv=submission_file,\n",
    "        thresholds_csv='/kaggle/input/image-matching-challenge-2025/train_thresholds.csv',\n",
    "        mask_csv=None if is_train else os.path.join(data_dir, 'mask.csv'),\n",
    "        inl_cf=0,\n",
    "        strict_cf=-1,\n",
    "        verbose=True,\n",
    "    )\n",
    "    print(f'Computed metric in: {time() - t:.02f} sec.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc1ac4",
   "metadata": {},
   "source": [
    "トレーニングデータで実行する場合のみ、提出ファイルに対して公式評価指標（mAA + クラスタリング精度）を算出します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112d305",
   "metadata": {},
   "source": [
    "## ✅ まとめ\n",
    "1. グローバル特徴（DINOv2）で類似画像選定\n",
    "\n",
    "2. 局所特徴（ALIKED+LightGlue）でマッチング\n",
    "\n",
    "3. Structure-from-Motion（COLMAP）でカメラポーズ推定\n",
    "\n",
    "4. 結果をsubmission形式で保存\n",
    "\n",
    "という一貫したパイプラインを組んでいる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6eda2d",
   "metadata": {},
   "source": [
    "## 🗺️ 全体処理のフローチャート（Mermaid記法による図解）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5693ad11",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart TD\n",
    "    A[スタート] --> B[依存関係のセットアップとモデル準備]\n",
    "    B --> C[画像の読み込みと前処理]\n",
    "    C --> D[DINOv2でグローバル特徴抽出]\n",
    "    D --> E[類似度に基づく画像ペアのショートリスト]\n",
    "    E --> F[ALIKEDで局所特徴抽出]\n",
    "    F --> G[LightGlueで特徴マッチング]\n",
    "    G --> H[COLMAP用DBにマッチデータをインポート]\n",
    "    H --> I[COLMAPによるRANSACとマッピング]\n",
    "    I --> J[カメラ姿勢（R, T）の推定]\n",
    "    J --> K[クラスタ割当と結果の保存]\n",
    "    K --> L[submission.csv の生成]\n",
    "    L --> M[スコア算出（トレーニング時のみ）]\n",
    "    M --> Z[終了]\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
